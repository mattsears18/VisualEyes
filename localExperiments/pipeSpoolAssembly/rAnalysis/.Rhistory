---
title: "Homework 07"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/mattsears/Google Drive/SCHOOL/PSYC 5741 - Stats/Homework 07")
```
```{r}
library(psych)
library(lmSupport)
library(car)
library(pwr)
source('http://psych.colorado.edu/~jclab/R/mcSummaryLm.R')
data = read.csv('../DataFiles/apgar.csv', header=TRUE, na.strings='.')
```
##1. In last week's homework, you estimated a model from the APGAR dataset, predicting APGAR scores from GESTAT. Now we want you to generate two different confidence intervals for the mean APGAR score, one based on Chapter 4 procedures (using the simple model that makes a constant prediction for everyone) and a second based on the conditional model that assumes that GESTAT is a useful predictor of APCAR scores.
```{r}
# Chapter 4 Model
(modelA4 = lm(APGAR ~ 1, data))
(mcSummary(modelA4))
```
$F_{crit;1;n-1}=4.00$
$MSE=\frac{SSE}{n-1}=\frac{260.983}{60-1}$
```{r}
(MSE = 260.983/(60-1))
```
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
$b_0\pm\sqrt{\frac{4.00*4.423}{60}}$
$b_0=\bar{APGAR}=6.683$
```{r}
(sqrt(4.00*4.423441/60))
```
Model 4 Confidence Interval:
$6.683\pm0.543$
[6.140 - 7.226]
```{r}
#Chapter 5 Model
data$GESTATC = data$GESTAT - mean(data$GESTAT)
(modelA5 = lm(APGAR ~ 1 + GESTATC, data))
(mcSummary(modelA5))
```
$F_{crit;1;n-2}=4.00$
$MSE=\frac{SSE}{n-2}=\frac{188.939}{60-2}$
```{r}
(MSE = 188.939/(60-2))
```
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
$b_0\pm\sqrt{\frac{4.00*3.257569}{60}}$
$b_0=\bar{APGAR}=6.683$
```{r}
(sqrt(4.00*3.257569/60))
```
Model 5 Confidence Interval:
$6.683\pm0.466$
[6.217 - 7.149]
##Using the formula for the confidence interval for the intercept (both in the unconditional model and in the conditional model) discuss how and why these two different confidence intervals differ from each other.
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
Given the formula above for the confidence interval, the confidence interval is dependent upon $F_{crit}$, $MSE$, and $n$. $n$ does not change and $F_{crit}$ is essentially the same for both models. $F_{crit}\approx4.00$. The Mean Squared Error (MSE) is the only variable that really changes and the MSE is in the numerator, so a smaller MSE will produce a smaller confidence interval. In this case, the Chapter 5 model has a smaller MSE than the Chapter 4 model, because the Chapter 5 model makes predictions based upon GESTAT (a good predictor) which leads to a smaller SSE.
$SSE_5=188.939<SSE_4=260.983$
It should be noted that a smaller $SSE$ does not always lead to a smaller confidence interval. If the sample size had been smaller (e.g. $n=5$) then it's possible that the Chapter 4 model would have had a smaller confidence interval.
##2. Imagine that an ideal APGAR score is 8. If a mother has a 40 week gestation period, one might expect the baby to have such an ideal APGAR score. But these are (hypothetically) data from disadvantaged mothers. Test whether the predicted APGAR score in these data for babies following a 40 week gestation period is different from 8. Do this by estimating both a model A and a model C making conditional predictions of APGAR from GESTAT. Model A should be one in which the intercept estimates the predicted APGAR score following a 40 week gestation period. Model C will fix that predicted APGAR score at 8. Present the resulting models, the relevant statistics for testing the null hypothesis that the best predicted value is 8, and a brief discussion of your conclusions.
Model A: $A_i=\beta_0+\beta_1Gdev_i+\varepsilon_i$
Model C: $A_i=B_0+\beta_1Gdev_i+\varepsilon_i$&nbsp;&nbsp;&nbsp;&nbsp;$where\:B_0=8$
Null Hypothesis ($H_0$): $\beta_0=8$
```{r}
data$GESTAT40 =  data$GESTAT - 40
(modelA = lm(APGAR ~ GESTAT40, data))
data$offset = 8
(modelC = lm(data$APGAR ~ 0 + data$GESTAT40, offset = data$offset))
plot(APGAR ~ GESTAT40, data, main="Centered GESTAT and APGAR")
abline(modelA, col='dodgerblue')
abline(8, coef(modelC), col='red', lty='dashed')
mcSummary(modelA)
```
Because Model A is centered around a GESTAT period of 40 weeks, the predicted intercept is the predicted APGAR score for a baby with a gestation perdiod of 40 weeks. From the output of `mcSummary()`, we see that the 95% confidence interval for the intercept (i.e. the predicted APGAR score for a gestation period of 40 weeks) is `[6.745 - 7.805]`. 8 is not included in this range, therefore we can reject the null hypothesis and conclude that the best predicted APGAR score for a baby with a gestation period of 40 weeks is not 8.
We can calculate F as a check (and for practice).
```{r}
(mcSummary(modelC))
```
$PRE=\frac{SSE_C-SSE_A}{SSE_C}$
```{r}
(PRE = (213.376 - 188.939) / 213.376)
```
$F=\frac{PRE/(PA-PC)}{(1-PRE)/(n-PA)}$
```{r}
PA = 2
PC = 1
(n = nrow(data))
(F = (PRE / (PA - PC)) / ((1 - PRE) / (n - PA)))
```
$F_{crit;1;58}=4.00$
$F=7.50>F_{crit}=4.00$, therefore reject null hypothesis. Predicted APGAR score for a gestation period of 40 weeks is not 8. CHECK.
#Matt Sears
View(data)
View(data)
View(mcSummary)
View(mcSummary)
---
title: "Homework 07"
output: pdf_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
setwd("/Users/mattsears/Google Drive/SCHOOL/PSYC 5741 - Stats/Homework 07")
```
```{r}
library(psych)
library(lmSupport)
library(car)
library(pwr)
source('http://psych.colorado.edu/~jclab/R/mcSummaryLm.R')
data = read.csv('../DataFiles/apgar.csv', header=TRUE, na.strings='.')
```
##1. In last week's homework, you estimated a model from the APGAR dataset, predicting APGAR scores from GESTAT. Now we want you to generate two different confidence intervals for the mean APGAR score, one based on Chapter 4 procedures (using the simple model that makes a constant prediction for everyone) and a second based on the conditional model that assumes that GESTAT is a useful predictor of APCAR scores.
```{r}
# Chapter 4 Model
(modelA4 = lm(APGAR ~ 1, data))
(mcSummary(modelA4))
```
$F_{crit;1;n-1}=4.00$
$MSE=\frac{SSE}{n-1}=\frac{260.983}{60-1}$
```{r}
(MSE = 260.983/(60-1))
```
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
$b_0\pm\sqrt{\frac{4.00*4.423}{60}}$
$b_0=\bar{APGAR}=6.683$
```{r}
(sqrt(4.00*4.423441/60))
```
Model 4 Confidence Interval:
$6.683\pm0.543$
[6.140 - 7.226]
```{r}
#Chapter 5 Model
data$GESTATC = data$GESTAT - mean(data$GESTAT)
(modelA5 = lm(APGAR ~ 1 + GESTATC, data))
(mcSummary(modelA5))
```
$F_{crit;1;n-2}=4.00$
$MSE=\frac{SSE}{n-2}=\frac{188.939}{60-2}$
```{r}
(MSE = 188.939/(60-2))
```
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
$b_0\pm\sqrt{\frac{4.00*3.257569}{60}}$
$b_0=\bar{APGAR}=6.683$
```{r}
(sqrt(4.00*3.257569/60))
```
Model 5 Confidence Interval:
$6.683\pm0.466$
[6.217 - 7.149]
##Using the formula for the confidence interval for the intercept (both in the unconditional model and in the conditional model) discuss how and why these two different confidence intervals differ from each other.
$b_0\pm\sqrt{\frac{F_{crit;1;n-1}MSE}{n}}$
Given the formula above for the confidence interval, the confidence interval is dependent upon $F_{crit}$, $MSE$, and $n$. $n$ does not change and $F_{crit}$ is essentially the same for both models. $F_{crit}\approx4.00$. The Mean Squared Error (MSE) is the only variable that really changes and the MSE is in the numerator, so a smaller MSE will produce a smaller confidence interval. In this case, the Chapter 5 model has a smaller MSE than the Chapter 4 model, because the Chapter 5 model makes predictions based upon GESTAT (a good predictor) which leads to a smaller SSE.
$SSE_5=188.939<SSE_4=260.983$
It should be noted that a smaller $SSE$ does not always lead to a smaller confidence interval. If the sample size had been smaller (e.g. $n=5$) then it's possible that the Chapter 4 model would have had a smaller confidence interval.
##2. Imagine that an ideal APGAR score is 8. If a mother has a 40 week gestation period, one might expect the baby to have such an ideal APGAR score. But these are (hypothetically) data from disadvantaged mothers. Test whether the predicted APGAR score in these data for babies following a 40 week gestation period is different from 8. Do this by estimating both a model A and a model C making conditional predictions of APGAR from GESTAT. Model A should be one in which the intercept estimates the predicted APGAR score following a 40 week gestation period. Model C will fix that predicted APGAR score at 8. Present the resulting models, the relevant statistics for testing the null hypothesis that the best predicted value is 8, and a brief discussion of your conclusions.
Model A: $A_i=\beta_0+\beta_1Gdev_i+\varepsilon_i$
Model C: $A_i=B_0+\beta_1Gdev_i+\varepsilon_i$&nbsp;&nbsp;&nbsp;&nbsp;$where\:B_0=8$
Null Hypothesis ($H_0$): $\beta_0=8$
```{r}
data$GESTAT40 =  data$GESTAT - 40
(modelA = lm(APGAR ~ GESTAT40, data))
data$offset = 8
(modelC = lm(data$APGAR ~ 0 + data$GESTAT40, offset = data$offset))
plot(APGAR ~ GESTAT40, data, main="Centered GESTAT and APGAR")
abline(modelA, col='dodgerblue')
abline(8, coef(modelC), col='red', lty='dashed')
mcSummary(modelA)
```
Because Model A is centered around a GESTAT period of 40 weeks, the predicted intercept is the predicted APGAR score for a baby with a gestation perdiod of 40 weeks. From the output of `mcSummary()`, we see that the 95% confidence interval for the intercept (i.e. the predicted APGAR score for a gestation period of 40 weeks) is `[6.745 - 7.805]`. 8 is not included in this range, therefore we can reject the null hypothesis and conclude that the best predicted APGAR score for a baby with a gestation period of 40 weeks is not 8.
We can calculate F as a check (and for practice).
```{r}
(mcSummary(modelC))
```
$PRE=\frac{SSE_C-SSE_A}{SSE_C}$
```{r}
(PRE = (213.376 - 188.939) / 213.376)
```
$F=\frac{PRE/(PA-PC)}{(1-PRE)/(n-PA)}$
```{r}
PA = 2
PC = 1
(n = nrow(data))
(F = (PRE / (PA - PC)) / ((1 - PRE) / (n - PA)))
```
$F_{crit;1;58}=4.00$
$F=7.50>F_{crit}=4.00$, therefore reject null hypothesis. Predicted APGAR score for a gestation period of 40 weeks is not 8. CHECK.
#Matt Sears
download <- read.csv("~/Downloads/download.csv")
View(download)
data = download
View(download)
View(download)
View(download)
View(data)
View(data)
cor
corr()
cor()
cor(data)
data = Filter(is.numeric, download)
View(data)
View(data)
cor(data)
cor = cor(data)
View(cor)
View(cor)
lastAnalysisExport <- read.csv("~/code/eyetracker/rAnalysis/lastAnalysisExport.csv")
View(lastAnalysisExport)
data = read.csv("./lastAnalysisExport.csv")
data = read.csv("lastAnalysisExport.csv")
require(lolcat)
require(psych)
require(lmSupport)
source('http://psych.colorado.edu/~jclab/R/mcSummaryLm.R')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Remove the single largest outlier data point (54)
# Note, point 54 was removed because it was known to have calibration issues
d = read.csv("ProportionData.csv", header = TRUE)
# Histograms of ALL data points - Less point (54)
hist.grouped(d$fix, main = "Proportion of Fixations on Hazards")
hist.grouped(d$norm_hri, main = "Proportion of Hazards Identified")
plot.default(x = d$fix, y = d$norm_hri, xlab = "Percentage of Fixations on Hazards", ylab = "Normalized Hazard Recognition Index", main = "Fixations on Hazards vs Hazards Identified")
summary.continuous(d)
# Use Cook's distance to determine if any outliers should be removed
fit = lm(norm_hri ~ fix, data = d)
plot(fit, pch = 18, col="red", which = c(4))
plot(fit, pch = 18, col="red", which = c(5))
(cds = sort(round(cooks.distance(fit), 5), decreasing = TRUE))
# Maximum Cook's distance of 0.13584 (point 7). No need to remove any additional data points.
# Univariate analysis (ANOVA)
fita = aov(norm_hri_high ~ fix, data = d)
Anova(fita)
# ANOVA (another way)
model = lm(norm_hri_high ~ fix, data=d)
mcSummary(model)
# Regression analysis
cor.spearman.rank(x1 = d$fix, x2 = d$norm_hri, conf.level = 0.95)
(r = cor(x = d$fix, y = d$norm_hri, method = "spearman"))
(rSquared = r^2)
require(lolcat)
require(psych)
require(lmSupport)
source('http://psych.colorado.edu/~jclab/R/mcSummaryLm.R')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
d = read.csv("ProportionData.csv", header = TRUE)
View(d)
View(d)
hist.grouped(d$fix, main = "Proportion of Fixations on Hazards")
hist.grouped(d$norm_hri, main = "Proportion of Hazards Identified")
require(lolcat)
install_github("burrm/lolcat")
install.packages("devtools")
install_github("burrm/lolcat")
require(devtools)
install_github("burrm/lolcat")
require(lolcat)
source('/Volumes/[C] My Boot Camp.hidden/Users/mattsears/Google Drive/Eye Tracking Software and Data/From Dylan/CSCE Conference 2017/Analysis/Analysis.R', echo=TRUE)
d = read.csv("ProportionData.csv", header = TRUE)
hist.grouped(d$fix, main = "Proportion of Fixations on Hazards")
hist.grouped(d$norm_hri, main = "Proportion of Hazards Identified")
plot.default(x = d$fix, y = d$norm_hri, xlab = "Percentage of Fixations on Hazards", ylab = "Normalized Hazard Recognition Index", main = "Fixations on Hazards vs Hazards Identified")
summary.continuous(d)
fit = lm(norm_hri ~ fix, data = d)
plot(fit, pch = 18, col="red", which = c(4))
plot(fit, pch = 18, col="red", which = c(5))
plot(fit, pch = 18, col="red", which = c(4))
(cds = sort(round(cooks.distance(fit), 5), decreasing = TRUE))
plot(fit, pch = 18, col="red", which = c(4))
plot(fit, pch = 18, col="red", which = c(5))
plot(fit, pch = 18, col="red", which = c(4))
fit = lm(norm_hri ~ fix, data = d)
(cds = sort(round(cooks.distance(fit), 5), decreasing = TRUE))
fita = aov(norm_hri_high ~ fix, data = d)
Anova(fita)
require(lolcat)
require(psych)
require(lmSupport)
source('http://psych.colorado.edu/~jclab/R/mcSummaryLm.R')
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# Remove the single largest outlier data point (54)
# Note, point 54 was removed because it was known to have calibration issues
d = read.csv("ProportionData.csv", header = TRUE)
model = lm(norm_hri_high ~ fix, data=d)
mcSummary(model)
cor.spearman.rank(x1 = d$fix, x2 = d$norm_hri, conf.level = 0.95)
(r = cor(x = d$fix, y = d$norm_hri, method = "spearman"))
(rSquared = r^2)
library(rstudioapi)
library(lmSupport)
library(ggplot2)
library(ggpmisc)
current_path = getActiveDocumentContext()$path
setwd(dirname(current_path ))
analysis = read.csv(file = "lastAnalysisExport.csv", header=TRUE)
data = Filter(is.numeric, analysis)
data = data[, -c(1:3)] # delete columns 1 through 3
cors2 = cor(data)^2
xName = "Indirect.Work.."
yName = "Total.Time"
formula = y ~ x
ggplot(data, aes_string(x = xName, y = yName)) +
geom_point() +
geom_smooth(method = "lm", formula = formula, se = TRUE) +
stat_poly_eq(aes(label =  paste(stat(eq.label), stat(rr.label), stat(adj.rr.label), sep = "~~~~")),
label.x.npc = "left", label.y.npc = 0.90,
formula = formula, rr.digits = 3, coef.digits = 4, parse = TRUE, size = 4)+
stat_fit_glance(method = 'lm',
method.args = list(formula = formula),
geom = 'text',
aes(label = paste("Slope P-value = ", signif(..p.value.., digits = 3), sep = "")),
label.x.npc = 'left', label.y.npc = 0.85, size = 4)
x = unlist(data[xName])
y = unlist(data[yName])
model = lm(y ~ x)
summary(model)
#plot(x, y)
#abline(model)
#legend("bottomright", bty="n", legend=paste("Adjusted R^2 = ", format(summary(model)$adj.r.squared, digits=4)))
